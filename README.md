# MLOps_Project
Final Project for MLOPS learning
---
This project serves as a capstone to the tutorial sessions taken concerning the work flow of Machine Learning Operations. 

In machine learning, once the dataset is acquired, MLOps Engineers typically go through the following steps
- Model Training
- Experiment Tracking
- Orchestration 
- Deployment 
- Monitoring

So let's go through my process at tackling this problem.

### The Dataset
After spending a weekend thinking of what dataset to use, finding my Eureka moment then later seeiing the potential flaws of using the chosen dataset, I decided to work on a simple and very common dataset that I had worked on in the past. I decided to go with a Wine Quality Dataset, where some characteristics of wine are gathered and matched to what professional wine taster deem to rate the selected wine.

Instead of splitting one dataset into train and test, I decided to get two different wine quality datasets to use for each. I wanted the training dataset to be as big as possible to improve the model.

I got both my datasets from kaggle, this is the link;

- Training Set: https://www.kaggle.com/datasets/subhajournal/wine-quality-data-combined

- Validation Set: https://www.kaggle.com/datasets/rajyellow46/wine-quality 


### 1. Model Training

Before we even start the project, if you're starting from scratch then you might want to get Python installed and create a virtual environment for this.

#### - Packages
For the packages and libraies used for this particular section, we would have to ```pip install``` them. Primarily;

- ```pandas``` for data manipulation
- ```matplotlib``` and ```seaborn``` for data visualization
- ```sklearn``` for model training

#### - Inspecting the Data

Following the code in the repository,a made a ```python notebook``` file and loaded the training data and examined it. A few interesting things and be gathered from the output below which was achieved by the following code 
```python 
wine_train_df.isnull().sum()
```
The output:
```
type                    0
fixed_acidity           0
volatile_acidity        0
citric_acid             0
residual_sugar          0
chlorides               0
free_sulfur_dioxide     0
total_sulfur_dioxide    0
density                 0
pH                      0
sulphates               0
alcohol                 0
quality                 0
dtype: int64
```

Frome the out put, there are no null values (a good thing). As much as this data looks almost perfect, there was a problem.


On a closer look at the data, we can see that almost all the columns are numerical except ```type```. Numerical data is crucial when it comes to working with Machine Learning Models.

To fix this problem, I changed the values of the ```type``` column from **categorical** to **numerical** using *One Hot Encoding*


```python
from sklearn.preprocessing import OneHotEncoder

#encoding the wine types as 1 and 0 with respect to red to be computated

encoder = OneHotEncoder(sparse=False)

encode_types = encoder.fit_transform(wine_train_df['type'].values.reshape(-1, 1))
encode_types
```

the code above changes the values in the ```type``` column from white and red to 1 and 0. Now red wine will be represented as 1 and white wine as 0


#### - Training

After we have gotten the data ready, the fun part begins, *Model Training*. 

Training the model wasn't as easy as I thought. The process was pretty straight forward but my issue came from the results, they were pretty bad and not even close to accurate but as suggested by my mentor, I read a few research papers concerning the topic and they helped a lot. Not only did they suggest better models to use but they also came up with better methods.
    
I initially, removed most of the columns kept the ones that mostly affected the quality of wine (which were not many) according to the heatmap below, which was also generated by my code

![](images/heatmap.png)

I even tried to combine some of them to make new columns. But the research papers used all of them and they got better results so I went down that route. I also had to split my data into independent variables and dependent variables(target variables)


Before I talk about my model performance, let me explain the models I used

- **DecisionTreeClassifier**: This is a basic machine learning algorithm used for both classification and regression tasks. It works by splitting the data into subsets based on certain conditions, and making decisions based on those splits. Each split forms a branch of the tree, and each leaf node represents an output
- **RandomForestRegressor**: This is a type of ensemble learning method that operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. It is particularly effective when dealing with large datasets with high dimensionality.
- **RandomForestClassifier**: Similar to RandomForestRegressor, RandomForestClassifier is also an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of classes (classification) or mean prediction (regression) of the individual trees. It's commonly used for both classification and regression tasks
- **ExtraTreesClassifier**: This is another type of ensemble learning method similar to RandomForestClassifier. The key difference between ExtraTreesClassifier and RandomForestClassifier is how they handle categorical features. While RandomForestClassifier can handle categorical features, ExtraTreesClassifier can only handle numerical features. Because of this, ExtraTreesClassifier can sometimes achieve better performance when dealing with purely numerical datasets like this one.
- **KNeighborsRegressor**: This is a type of instance-based learning algorithm used for regression tasks. It works by finding the 'k' closest instances to the new instance and predicting the output value based on the average or median output of these 'k' closest instances. The 'k' closest instances are determined based on a distance measure, often the Euclidean distance


Now that I had my data ready, I begun the training. Through my research, I picked 5 models to use and this was the outcome on the training set.

    RandomForestRegressor
    RMSE:  {0.027071980054071056}
    Accuracy:  {0.999037171881531}
---
    RandomForestClassifier
    RMSE:  {0.0}
    Accuracy:  {1.0}
---
    ExtraTreesClassifier
    RMSE:  {0.0}
    Accuracy:  {1.0}
---
    DecisionTreeClassifier
    RMSE:  {0.0}
    Accuracy:  {1.0}
---
    KNeighborsRegressor
    RMSE:  {0.16338349530598967}
    Accuracy:  {0.9649308622642955}

**Now hold on**, let's not get too excited yet. Yes, these results look pretty impressive. Most of them are perfect but let's not forget that we haven't tested our model on the validation dataset yet.


#### - Validation

Now that we have our models trained, let's bring in the validation set. To avoid going through whole data investigation thing with the second dataset, I made it simple function to do it for me.

```python
def read_dafaframe(filename):
    df = pd.read_csv(filename)

    encoder = OneHotEncoder(sparse=False)

    encoded_types = encoder.fit_transform(df['type'].values.reshape(-1, 1))

    label= ['red','white']
    wine_types = pd.DataFrame(encoded_types, columns= label)

    df['red_wine'] = wine_types['red']

    return df
```


On working with the second dataset, I noticed something.

    #   Column                Non-Null Count  Dtype  
    ---  ------                --------------  -----  
    0   type                  6497 non-null   object 
    1   fixed_acidity         6487 non-null   float64
    2   volatile_acidity      6489 non-null   float64
    3   citric_acid           6494 non-null   float64
    4   residual_sugar        6495 non-null   float64
    5   chlorides             6495 non-null   float64
    6   free_sulfur_dioxide   6497 non-null   float64
    7   total_sulfur_dioxide  6497 non-null   float64
    8   density               6497 non-null   float64
    9   pH                    6488 non-null   float64
    10  sulphates             6493 non-null   float64
    11  alcohol               6497 non-null   float64
    12  quality               6497 non-null   int64  
    13  red_wine              6497 non-null   float64

If you take a look at the Non-Null Count column, you will see that our data has some null values in there.

There are different ways to deal with null values depending on the dataset. You can fill them with the mean value, fill them with 0 or remove those rows completely but the last method will result in other data being lost. 

Based on the dataset, I decided to fill them with the mean values 

```python
#filling in the mean values

for col, value in validation_df.items():
 if col != 'type':
    validation_df[col] = validation_df[col].fillna(validation_df[col].mean())
validation_df.isnull().sum()
```

Now that the second dataset is ready, let's test out our models on it. I dropped out decision tree to have fewer models to filter out in the next stage of the project

    RandomForestRegressor
    RMSE: 0.08215843046127176
    Accuracy: 0.9911470362380227
---
    RandomForestClassifier
    RMSE: 0.06681016826772575
    Accuracy: 0.9941457731172564
---
    ExtraTreesClassifier
    RMSE:  0.0632601533851334
    Accuracy:  0.9947513827947816
---
    KNeighborsRegressor
    RMSE: 0.260202026623686
    Accuracy: 0.9112014716834054


Well there you have it. These results aren't as perfect as the ones before but they are still super accurate. Great, our models are ready for the next stage, Experiment Tracking

### 2. Experiment Tracking

Now when it comes to getting the best results from a model, we have to look at the parameters used for the various models and see which changes can affect and improve our model. This is known as model tuning. But that sounds like a lot of work, finding the best combination of parameters to use, trying different values and going through a trial and error phase.

Luckily, there's no need to go through all of that because there is a library that does that for us and it is called ```mlflow``` and that is what we will be using in this section.

This is called **experiment tracking** and we will need to install a few more dependencies for this one. If you navigate to the **experiment tracking** folder, there is a file called **requirements.txt** and that's where our essential packages are. Once you are in this directory, go down to the terminal on your IDE and run the command ```pip install -r requirements.txt``` to install all these packages at once.


Most of the imports in the new ```notebook``` file will be the basic imports used in the **Model Training** section with a few extra.

The first import will ```mlflow``` itself.
```python
import mlflow

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("wine-quality-experiment")
```

After running this cell in the ```.ipynb``` file, two things appear in the directory. A folder called ```mlruns``` and a file called ```mlflow.db``` . The ```mlruns``` folder is where the mlflow **runs** are stored, we'll get to that later and the ```mlflow.db``` serves as the database for our mlflow project. If you visit mlflow from a directory that does not have the ```mlflow.db``` used before, your data and your runs may not be there anymore.


So after that, we go back to what we did before. Data preparing, and model training. Luckily, the data processing function I made in the previous section came in handy here and the model training wasn't that tedious. After training a model, if the results are good, it is sometimes good to save the trained model using ```pickle``` which is what I did.


For this section, I filtered out the models I used and decided to go with ```RandomForestClassifier``` and ```ExtraTreesClassifier``` 

After training the models, ```ExtraTreesClassifier``` gave me the best results so I logged that into ```mlflow```

```python
with mlflow.start_run():

    mlflow.set_tag("developer","Sven")

    mlflow.log_param("train-data-path", "../wine_data/train_wine_data.csv")
    mlflow.log_param("valid-data-path", "../wine_data/test_wine_data.csv")

    n_estimators = 100
    mlflow.log_param('n_estimators',n_estimators)

    etc = ExtraTreesClassifier(n_estimators=n_estimators)

    etc.fit(X_train, y_train)
    y_pred_etc = etc.predict(X_val)

    rmse = mean_squared_error(y_val,y_pred_etc,squared=False)
    accuracy = r2_score(y_val,y_pred_etc)
    
    print('ExtraTreesClassifier')
    print (f'RMSE: ',{rmse})
    print(f'Accuracy: ',{accuracy})

    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("accuracy", accuracy)
    mlflow.set_tag("model", "ExtraTreesClassifier")

    mlflow.log_artifact("models/etc.bin",artifact_path="sklearn_model")
    
    mlflow.sklearn.log_model(etc,"model")
```

What this does is basically log my model parameters and metrics into ```mlflow``` after the model has been trained and tested, pretty basic, but what we wanted was to get a combination of parameters that gave us the best results. For that there a few more imports to make

```python
import xgboost as xgb
from hyperopt import  fmin, tpe, hp, STATUS_OK, Trials
from hyperopt.pyll import scope
```

The imports that will help us with the parameter tuning is called ```hyperopt``` and I added another model to use that is not part of the ```sklearn``` library like the previous ones I used


After the mlflow functions are ready and have been run, like in the code, you should see output that looks something like this:

    [0]	validation-rmse:0.77562                           
    [1]	validation-rmse:0.68940                           
    [2]	validation-rmse:0.61295                           
    [3]	validation-rmse:0.54542                           
    [4]	validation-rmse:0.48524                           
    [5]	validation-rmse:0.43219                           
    [6]	validation-rmse:0.38561                           
    [7]	validation-rmse:0.34440                           
    [8]	validation-rmse:0.30808                           
    [9]	validation-rmse:0.27634                           
    [10]validation-rmse:0.24822                          
    [11]validation-rmse:0.22376                          
    [12]validation-rmse:0.20239  
    .....


This means that our runs are being logged into ```mlflow```. To view this better, we would have to go to the ```mlflow``` UI by navigating to the directory where your ```.ipynb``` file is along with the ```mlruns``` folder and ```mlflow.db``` and run this command in your terminal;

    mlflow ui --backend-store-uri sqlite:///mlflow.db

From personal experience, it is best to do all your work in a virtual environment.

After running the command in the terminal, you should see something like this if you're running the command for the first time;

![](images/Default.png)

I have made an experiment already and logged some runs so this is how mine looks like:

![](images/wine-quality-experiment.png)


Great, the logs appeared on mlflow. It is best to compare the runs with the tags selected and see which ones fit the requirements. We can also take a single run and see what it entails by jusy clicking on one of the run names which should take you to a page that looks something like this: 

![](images/delightful-wale.png)

Now we get to see all the information relating to the run. We see the parameters, tags and metrics. It is good to log this kind of imformation to mlflow. 


After selecting the best parameters, we can go a step furthur by using the ```autolog``` method.

```python
mlflow.xgboost.autolog()
```

Here, we do not have to log as many things as we did before, mlflow will do that for us. Before using the ```autolog``` method, this is how our parameters would have looked like;

![](images/7-parameters.png)

Now after the ```autolog``` method, take a look at th parameters now;

![](images/12-parameters.png)
![](images/full-12.png)

Now we have 12, instead of 7 parameters. We also get some information about our model logged.


![](images/log-model-artifacts.png)

It comes along with instructions on how to use the model for prediction.

We can also ```log artifacts``` which is some information about the specfic model saved. For ExtraTreesClassifier, my code was something like this because I saved my model as ```etc.bin```;

```python
mlflow.log_artifact(local_path="models/etc.bin", artifact_path="models_pickle")
```

And for ```xgboost``` , this is how my code was;

```python
mlflow.log_artifact(local_path="models/xgb.bin", artifact_path="models_pickle")
```

And this was the result:

![](images/log-artifacts.png)

A good thing about logging models is that when you examin the various results, you can also compare based on models:

![](images/different-models.png)

#### Model Registry

The last bit about the experiment tracking is called **model registry**, where you can register your models, which is essentially storing it somewhere as a chosen model given a version number. This way, you can always update your selected models according to time and you can choose which version is best. You can register a model by hitting the blue button on the far left (when you open a run) that says **Register Model**.


This will take you to your registered models. If you are following along, you might not have any models registered until you actually register them.

![](images/model-registry.png)

Model Registering can also be done in code as well as in the mlflow ui. It really depends on preference or maybe on what the task demands.

You can also assing **stages** to the versions. I found the best results to come from *Version 1* and that is why I have put it in **Production**

![](images/versioning.png)


### 3. Orchestration

Okay, a lot of progress so far but going further, there might be a lot of things to do to the model. We might want our model to run on at a scheduled time or perhaps observe what is going on when it runs and any possible failures.

This can be possible with another library called **prefect**. Prefect alows for orchestration and observation of our workflows. Prefect is a flexible, open source Python framework to turn standard pipelines into fault-tolerant dataflows.

#### Installation

In this directory, there is also a ```requirements.txt``` in there but in addition to that, I sugest running ```pip install -U prefect``` in the terminal to get the updated version of prefect.

Prefect has lots of neat features. It runs with an Orchestartion API used by the server to work with metadata, comes with a sqlite database that is configured upon installation and a visually appealing UI.

Before I start with this section, let me clarify a few things to make sure we are on the same page.

- **Task:** A task is basically a function serving as a prefect unit.
- **Flow:** A flow is a 'container' where the tasks can be called and sent to the prefect UI.
- **Sub-flow:** A sublflow is a flow called by another flow.

Okay, let's get down to business. To run the prefect server, head down to your IDE terminal and run this:

    prefect server start

You would have to open another terminal because as long as the prefect server is running, that particular server will be unsuable. 

Once you run this command, you will be provided a link you can follow to the UI where you should see something like this:

![](images/Prefect-home.png)

If you are running the prefect server for the first time, your's might look a bit more bare.

#### Logging Flows

As usual, we beggin with some imports

```python
from prefect import flow, task
```
as well as our previous imports

To log the model runs as a flow on prefect, I turned all the important parts into functions or in this case, tasks. The way to turn a function into a task or a flow is by simply adding ```@task``` or ```@flow``` decorator

For instance, the function used before to read the dataframe becomes this: 

```python
@task(retries=4)
def read_dafaframe(filename):
    df = pd.read_csv(filename)

    le = LabelEncoder()
    df['type'] = le.fit_transform(df['type'])

    encoder = OneHotEncoder()
    encoded_types = encoder.fit_transform(df['type'].values.reshape(-1, 1))

    label = le.classes_.tolist()
    wine_types = pd.DataFrame(encoded_types.toarray(), columns=label)

    df = pd.concat([df, wine_types], axis=1)

    for col, value in df.items():
        if col != 'type':
            df[col] = df[col].fillna(df[col].mean())

    return df
```

The decorators can also take in arguments. For instance the decorator in this code has an argument that says ```retries=4``` , meaning that it should try again upto 4 times in the case of failure.

After I run my python script containing my tasks and flows, this what appeared on the UI:

![](images/first-run.png)

The flow run shows logs in the terminal as it runs:

![](images/first-run-logs.png)

The flow run also comes with some information

![](images/first-run-details.png)
![](images/first-run-details-logs.png)

#### Deployment

With prefect, it is also possible to deploy your work flows when they are ready.


    prefect deploy 'path to your python file':main_flow -n 'give it a name' -p 'name of work pool'


the ```'path to your python file'``` is basically the python files, where your tasks and flows are. You would also have to give this deployment a name and assign it to a pool so you'll have to make a pool. 

![](images/work-pool.png)

Once you do that, you can deploy. I decided to schedule my deployments within intervals.

![](images/scheduled%20deployments.png)


#### AWS

It is also possible to include AWS blocks as an alternative storage unit in prefect either from the UI or from the code if you already have an existing S3 bucket on the AWS Console

![](images/aws-s3-block.png)
![](images/aws-s3-block-2.png)


We can also keep our data in the aws bucket 


![](images/upload-to-aws.png)


#### Some Best Practices


Now that we have come this far, one important thing we must consider is **testing** in implementing and **CI/CD pipeline**.

- Testing: Here I implemented some testing using ```unittest``` to test each of the important aspects of the process that I have put into simple functions. This makes it easier to catch possible errors and fix them before deployment and production.

![](images/Unit_test1.png)

All I have to do is gather my functions and call them in a test file with various syntax to help me test


```python
class TestReadDataFrame(unittest.TestCase):

    def setUp(self):
        print('test started')

    def test_read_csv(self):
        df = read_dataframe(train_path)
        self.assertEqual(df.shape, (32485, 13))

        expected_cols = ['fixed_acidity', 'volatile_acidity', 
                        'citric_acid', 'residual_sugar', 
                        'chlorides', 'free_sulfur_dioxide',  
                        'total_sulfur_dioxide', 'density', 'pH', 
                        'sulphates', 'alcohol', 'quality'] 
        
        self.assertCountEqual(df[expected_cols].columns.tolist(), expected_cols)

    def test_label_encoding(self):
        df = read_dataframe(train_path)
        actual_df = pd.read_csv(train_path)
        # Count instances of each type 
        white_count = (actual_df['type'] == 'white').sum()  
        red_count = (actual_df['type'] == 'red').sum()
        # Check encoded columns match counts
        encoded_white = (df['type'] == 1).sum() 
        encoded_red = (df['type'] == 0).sum()
        
        # Compare counts between original and encoded
        self.assertEqual(white_count, encoded_white)
        self.assertEqual(red_count, encoded_red)

    
    def test_missing_values(self):
         df = read_dataframe(train_path)  
         # Check missing values
         self.assertFalse(df.isnull().any().any())
            


    def tearDown(self):
        print('test ended')

```

This particular test was to test the ```read_dataframe``` function. It makes sure it has the right columns, make sure the One Hot Encoding worked well and matches to the original values and lastly, it checks for missing values.

VS code also has a buit in UI component that makes viewing these test results easier. If you look at the left hand side of the image above, You will see the IDE showing that the tests have run sucessfully.


- CI/CD pipeline: The CI/CD (Continuos Integration/ Continuos Deployment)pipline  is a way to automate the whole process and keep it up to date by continuosly. It refers to pipeline continuous integration, which consists of automated building and testing of our code and model ,and pipeline continuous delivery, which consists of automated pipeline deployment for continuous training and delivery of ML models.

This was made possible with the help of ```github```. That was where I deployed it to and it did all my testing for me and indeed, I did have some errors and problems I had too fix. To get this up and running, you would need to make a ```.github``` directory and within that, make another folder called ```workflows```. Within the ```workflows``` directory, you need to create a ```github-actions-demo.yml``` file with the following content:


```yml
name: Model Training and Validation

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepchecks seaborn pandas numpy matplotlib scikit-learn mlflow xgboost hyperopt

      - name: Upload Artifacts 
        uses: actions/upload-artifact@v2
        with: 
          name: wine_data
          path: |
            wine_quality_training/wine_data/train_wine_data.csv
            wine_quality_training/wine_data/test_wine_data.csv
                  
      - name: Download Artifacts
        uses: actions/download-artifact@v2
        with:
          name: wine_data 
          path: |
            wine_quality_training/wine_data/train_wine_data.csv
            wine_quality_training/wine_data/test_wine_data.csv

      - name: List Downloaded Files
        run: |
          ls -R wine_quality_training/wine_data

      - name: Train Model
        run: python wine_quality_training/wine_training.py

      - name: Validate Model Performance
        run: python experiment_tracking/wine-tracking.py

      - name: Archive Deepchecks Results
        uses: actions/upload-artifact@v3
        with:
          name: deepchecks results
          path: experiment_tracking/mlruns/1/2b885cabc1f8435dbdd6c684891242ac/artifacts/model/MLmodel

```

This installs deepchecks, which is what we will use for our integration. It also goes through the needed files and runs them on ```github```. This happens on every ```push``` to give us something like this:

![](images/CI_CD1.png)
 

 And now we can go through and see our logs

![](images/CI_CD2.png)


### 1. Deployment

So with a functioning model like, it would be a shame to just leave it unused. Model deployment was discussed in previous sections but there are other ways where you can actually input the values for the independent variables and it will predict the target variable for you. 


Now depending on your use case, there are various means to deploy your model. It can be with in Batches, a Web Service or a Streaming service. For this particular use case, I think the best way to go is with a Web Service mostly because the quality of wine isn't really something you need to know on the go. You can get it whenever you decide to access it.


So with the Web Service, I initially wanted to use **Flask** but was advised to use a better alternative called **FastAPI** , which was faster and provided me with a UI to interact with. So for this section, I made a new directory called **deployment** and navigated to it then I installed some dependencies.

    pip install fastapi uvicorn

To install **FastAPI** and **uvicorn** (will be used to run the web app)


So all the times I used pickle to save the models, now is a good time to use those.


```python

# 1. Library imports
import uvicorn
from fastapi import FastAPI
from Quality import WineQuality
import numpy as np
import pickle
import pandas as pd
# 2. Create the app object
app = FastAPI()
pickle_in = open("rfc.pkl","rb")
classifier=pickle.load(pickle_in)

# 3. Index route, opens automatically on http://127.0.0.1:8000
@app.get('/')
def index():
    return {'message': 'Hello, World'}

# 4. Expose the prediction functionality, make a prediction from the passed
@app.post('/predict')
def predict_quality(data:WineQuality):

    fixed_acidity = data.fixed_acidity
    volatile_acidity = data.volatile_acidity
    citric_acid = data.citric_acid
    residual_sugar = data.residual_sugar
    chlorides = data.chlorides
    free_sulfur_dioxide = data.free_sulfur_dioxide
    total_sulfur_dioxide = data.total_sulfur_dioxide
    density = data.density
    pH = data.pH
    sulphates = data.sulphates
    alcohol = data.alcohol
    red_wine = data.red_wine

        # Convert the features to a NumPy array
    features = np.array([[fixed_acidity, volatile_acidity, citric_acid, residual_sugar,
                          chlorides, free_sulfur_dioxide, total_sulfur_dioxide, density,
                          pH, sulphates, alcohol, red_wine]])

    prediction = classifier.predict(features)

    prediction = prediction[0].item()

    return {
        'prediction': prediction
    }

# 5. Run the API with uvicorn
if __name__ == '__main__':
    uvicorn.run(app, host='127.0.0.1', port=8000)
    
```

Some part of this might look farmiliar because we are taking the columns from our featured data set. I basically used pickle to import our model, then made some routes with **FastAPI** ,one being a welcome page that just says *Hello World* and another that will predict. Before I talk about how it works, you would notice from the code that our predict route has a funcion that calls ```WineQuality``` from thin airðŸ¤”. Well that function is from an imported class I created called ```Quality``` that uses a library called ```pydantic``` which makes it very easy to integrate data science or machine learning works with **FastAPI**, let me show the class:


```python
from pydantic import BaseModel
class WineQuality(BaseModel):
   
   fixed_acidity:         float
   volatile_acidity:      float
   citric_acid:           float
   residual_sugar:        float
   chlorides:             float
   free_sulfur_dioxide:   float
   total_sulfur_dioxide:  float
   density:               float
   pH:                    float
   sulphates:             float
   alcohol:               float
   red_wine:              float
```


So you might think, like a typical **Flask** application, the way to get to the predictions is by adding ```"/predict"``` to the route but let's see what happens when we do that:

![](images/predict_route.png)

Our message is there but no predictions and no way of inputting values for our independent variables but **FastAPI** has a way of doing that. Instead of  ```"/predict"``` , we do  ```"/docs"``` instead and we get this:


![](images/FastAPI_docs.png)


A nice looking UI. And we can also see a json format of our independent variables so we can input some values.


![](images/FastAPI_json.png)


And we can also get a prediction:

![](images/FastAPI_prediction.png)


And we can confirm from out data, looking at the very first row, we can see that the predictions match the independent variables. I tried with a few other rows and got the right predictions, if you would like to try it too, you can pick a row from the dataset and use the values in there to confirm the predictions:

![](images/FastAPI_proof.png)


#### Load Testing

After using a web service to deploy the model, it is also helpful so test how the web app will fair when multiple users utilize it. With this, a very helpful tool is ```locust```. Locust is an open source performance/load testing tool for HTTP and other protocols. Its developer friendly approach allows you to define your tests in regular Python code. Locust tests can be run from command line or using its web-based UI.


To use ```locust``` , we will first have to install it with

    pip install locust

After that, I made new file in my ```deployment``` directory called ```locustfile.py``` and modified the initial FastAPI file.



```python
from locust import HttpUser, task, between
import json
import random

class MyUser(HttpUser):
    wait_time = between(1, 5)  # Time between requests

    @task(1)
    def index(self):
        self.client.get("/")

    @task(2)
    def predict(self):
        payload = {
           "fixed_acidity": random.uniform(3.8, 15.9),
            "volatile_acidity": random.uniform(0.08, 1.58),
            "citric_acid": random.uniform(0.0, 1.66),
            "residual_sugar": random.uniform(0.6, 65.8),
            "chlorides": random.uniform(0.009, 0.611),
            "free_sulfur_dioxide": random.uniform(1, 289.0),
            "total_sulfur_dioxide": random.uniform(6.0, 440.0),
            "density": random.uniform(0.987110, 1.038980),
            "pH": random.uniform(2.720, 4.010),
            "sulphates": random.uniform(0.220, 2.000),
            "alcohol": random.uniform(8.0, 14.9),
            "red_wine": random.choice([0, 1])
        }

        headers = {"Content-Type": "application/json"}

        response = self.client.post("/predict", data=json.dumps(payload), headers=headers)

        print(response.text)

    @task(3)
    def predict_with_invalid_data(self):
        # Send a POST request with invalid data to trigger a potential error
        invalid_payload = {"invalid_field": "invalid_value"}
        headers = {"Content-Type": "application/json"}

        response = self.client.post("/predict", data=json.dumps(invalid_payload), headers=headers)

        print(response.text)

``` 

This is the locust file for the prediction web service deployment. I assigned the values of the rows to random values between the minimum and maximum ranges to simulate multiple users.


To run this, head to the terminal and type out ```locust -f {name of file}``` and hit enter. A link should be provided and you will be taking to the locust UI.


![](images/locust_home.png)

Here, you have the option to select the number of users. You also have to provide the host address. 


Then we will see a table showing the model:


![](images/locust_chart.png)


Locust also shows some charts as the system is running:


![](images/locust_graph1.png)
![](images/locust_graph2.png)